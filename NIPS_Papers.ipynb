{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing NIPS papers using LDA topic modeling\n",
    "\n",
    "The LDA code is adapted from Jordan Barber's blog post [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url= 'https://nips.cc/Conferences/2015/AcceptedPapers'\n",
    "page = urllib2.urlopen(url)\n",
    "soup = BeautifulSoup(page.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get each paper title by finding every span with `class=larger-font`. Figuring this out required looking at the html source of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = soup.findAll(\"span\", {\"class\": \"larger-font\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can pull out the text content of each of those spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = [ti.contents[0] for ti in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    raw = text.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [tokenize(i) for i in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look a the first 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'double',\n",
       "  u'nothing',\n",
       "  u'multiplicative',\n",
       "  u'incentive',\n",
       "  u'mechanisms',\n",
       "  u'crowdsourcing'],\n",
       " [u'learning', u'symmetric', u'label', u'noise', u'importance', u'unhinged'],\n",
       " [u'algorithmic', u'stability', u'uniform', u'generalization'],\n",
       " [u'adaptive',\n",
       "  u'low',\n",
       "  u'complexity',\n",
       "  u'sequential',\n",
       "  u'inference',\n",
       "  u'dirichlet',\n",
       "  u'process',\n",
       "  u'mixture',\n",
       "  u'models'],\n",
       " [u'covariance',\n",
       "  u'controlled',\n",
       "  u'adaptive',\n",
       "  u'langevin',\n",
       "  u'thermostat',\n",
       "  u'large',\n",
       "  u'scale',\n",
       "  u'bayesian',\n",
       "  u'sampling']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a dictionary of tokenized titles, and convert that into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.011732983793008859, u'learning'),\n",
       "  (0.010482399776412965, u'analysis'),\n",
       "  (0.010342719435481698, u'algorithms'),\n",
       "  (0.0096651234244324539, u'via'),\n",
       "  (0.0094983256782661969, u'online'),\n",
       "  (0.009251637426688105, u'fast'),\n",
       "  (0.008841814391757373, u'stochastic'),\n",
       "  (0.0085808337414099382, u'neural'),\n",
       "  (0.0084995785011522922, u'high'),\n",
       "  (0.0072877433628405815, u'deep')],\n",
       " [(0.032258508362300202, u'learning'),\n",
       "  (0.019083355907547502, u'inference'),\n",
       "  (0.014040426820267148, u'optimization'),\n",
       "  (0.0094000730496172375, u'variational'),\n",
       "  (0.0091987922635342265, u'linear'),\n",
       "  (0.0086692262732991007, u'convex'),\n",
       "  (0.0081830125809068139, u'stochastic'),\n",
       "  (0.0071314117398587236, u'methods'),\n",
       "  (0.0071232912690603247, u'descent'),\n",
       "  (0.0070634589734472758, u'via')],\n",
       " [(0.029560009462343204, u'models'),\n",
       "  (0.021667680663985708, u'networks'),\n",
       "  (0.018752001998676174, u'learning'),\n",
       "  (0.010854141614568602, u'using'),\n",
       "  (0.010815113356399364, u'neural'),\n",
       "  (0.010013338409413276, u'optimization'),\n",
       "  (0.0088322874598608936, u'recurrent'),\n",
       "  (0.0086094076583379826, u'time'),\n",
       "  (0.0081919993615905563, u'sample'),\n",
       "  (0.0075879422416037946, u'efficient')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldamodel.show_topics(formatted=False)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analysis</td>\n",
       "      <td>inference</td>\n",
       "      <td>networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>optimization</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>via</td>\n",
       "      <td>variational</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online</td>\n",
       "      <td>linear</td>\n",
       "      <td>neural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fast</td>\n",
       "      <td>convex</td>\n",
       "      <td>optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stochastic</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>recurrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neural</td>\n",
       "      <td>methods</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>high</td>\n",
       "      <td>descent</td>\n",
       "      <td>sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>via</td>\n",
       "      <td>efficient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0             1             2\n",
       "0    learning      learning        models\n",
       "1    analysis     inference      networks\n",
       "2  algorithms  optimization      learning\n",
       "3         via   variational         using\n",
       "4      online        linear        neural\n",
       "5        fast        convex  optimization\n",
       "6  stochastic    stochastic     recurrent\n",
       "7      neural       methods          time\n",
       "8        high       descent        sample\n",
       "9        deep           via     efficient"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({topic:[i[1] for i in t] for topic,t in enumerate(topics)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
