{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing NIPS papers using LDA topic modeling\n",
    "\n",
    "The LDA code is adapted from Jordan Barber's blog post [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url= 'https://nips.cc/Conferences/2015/AcceptedPapers'\n",
    "page = urllib2.urlopen(url)\n",
    "soup = BeautifulSoup(page.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get each paper title by finding every span with `class=larger-font`. Figuring this out required looking at the html source of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = soup.findAll(\"span\", {\"class\": \"larger-font\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can pull out the text content of each of those spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = [ti.contents[0] for ti in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    raw = text.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [tokenize(i) for i in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look a the first 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'double',\n",
       "  u'nothing',\n",
       "  u'multiplicative',\n",
       "  u'incentive',\n",
       "  u'mechanisms',\n",
       "  u'crowdsourcing'],\n",
       " [u'learning', u'symmetric', u'label', u'noise', u'importance', u'unhinged'],\n",
       " [u'algorithmic', u'stability', u'uniform', u'generalization'],\n",
       " [u'adaptive',\n",
       "  u'low',\n",
       "  u'complexity',\n",
       "  u'sequential',\n",
       "  u'inference',\n",
       "  u'dirichlet',\n",
       "  u'process',\n",
       "  u'mixture',\n",
       "  u'models'],\n",
       " [u'covariance',\n",
       "  u'controlled',\n",
       "  u'adaptive',\n",
       "  u'langevin',\n",
       "  u'thermostat',\n",
       "  u'large',\n",
       "  u'scale',\n",
       "  u'bayesian',\n",
       "  u'sampling']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a dictionary of tokenized titles, and convert that into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.021683658123877118, u'learning'),\n",
       "  (0.017655087068210285, u'neural'),\n",
       "  (0.016952136252133005, u'networks'),\n",
       "  (0.0078411199663377361, u'efficient'),\n",
       "  (0.0078132540497462665, u'convolutional'),\n",
       "  (0.0076511674247561282, u'via'),\n",
       "  (0.0074118556495606476, u'prediction'),\n",
       "  (0.0074077285192164118, u'bayesian'),\n",
       "  (0.0074000132306674691, u'linear'),\n",
       "  (0.006436682404842335, u'data')],\n",
       " [(0.039029094467679597, u'learning'),\n",
       "  (0.017242054421422709, u'models'),\n",
       "  (0.01322435734987213, u'stochastic'),\n",
       "  (0.0092802367519656016, u'gaussian'),\n",
       "  (0.009205980998052251, u'sparse'),\n",
       "  (0.0086590219476347513, u'using'),\n",
       "  (0.0086061141425646161, u'deep'),\n",
       "  (0.0083293929756830826, u'via'),\n",
       "  (0.0079932981561969107, u'optimization'),\n",
       "  (0.0072048578179862112, u'sample')],\n",
       " [(0.018104368297777656, u'inference'),\n",
       "  (0.01605922533538762, u'optimization'),\n",
       "  (0.010295478839678457, u'variational'),\n",
       "  (0.0094634453838710331, u'bounds'),\n",
       "  (0.0088129537506973499, u'fast'),\n",
       "  (0.0076534943162640673, u'linear'),\n",
       "  (0.0074809142985172586, u'models'),\n",
       "  (0.0066440808562850612, u'algorithms'),\n",
       "  (0.0063067456140954766, u'robust'),\n",
       "  (0.0062400600458419235, u'networks')]]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldamodel.show_topics(formatted=False)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neural</td>\n",
       "      <td>models</td>\n",
       "      <td>optimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>networks</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>variational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficient</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convolutional</td>\n",
       "      <td>sparse</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>via</td>\n",
       "      <td>using</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prediction</td>\n",
       "      <td>deep</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bayesian</td>\n",
       "      <td>via</td>\n",
       "      <td>algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>optimization</td>\n",
       "      <td>robust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data</td>\n",
       "      <td>sample</td>\n",
       "      <td>networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2\n",
       "0       learning      learning     inference\n",
       "1         neural        models  optimization\n",
       "2       networks    stochastic   variational\n",
       "3      efficient      gaussian        bounds\n",
       "4  convolutional        sparse          fast\n",
       "5            via         using        linear\n",
       "6     prediction          deep        models\n",
       "7       bayesian           via    algorithms\n",
       "8         linear  optimization        robust\n",
       "9           data        sample      networks"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({topic:[i[1] for i in t] for topic,t in enumerate(topics)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
